% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tournament.R
\name{tournament}
\alias{tournament}
\title{Tournament - Model comparison}
\usage{
tournament(
  formula = NULL,
  data = NULL,
  ...,
  method = "WAIC",
  winning_criteria = NULL
)
}
\arguments{
\item{formula}{an object of class "formula", with discharge column name as response and stage column name as a covariate.}

\item{data}{data.frame containing the variables specified in formula.}

\item{...}{optional arguments passed to the model functions. Also, if data and formula are NULL, one can either add a previously created tournament object or four model objects of types "gplm", "gplm0", "plm" and "plm0". This runs the tournament for the input models and prevents running all four models explicitly.}

\item{method}{a string specifying the method used to estimate the predictive performance of the models. The allowed methods are "WAIC", "DIC" and "Posterior_probability".}

\item{winning_criteria}{a numerical value which sets a threshold which the more complex model in each model comparison must exceed to be deemed the more appropriate model. See the Details section.}
}
\value{
An object of type "tournament" with the following elements
\describe{
 \item{\code{contestants}}{model objects of types "plm0","plm","gplm0" and "gplm" being compared.}
 \item{\code{winner}}{model object of the tournament winner.}
 \item{\code{summary}}{a data frame with information on results of the different games in the tournament.}
 \item{\code{info}}{specifics about the tournament; the overall winner; the method used; and the winning criteria.}
}
}
\description{
tournament compares four rating curve models of different complexities and determines the model that provides the best fit of the data at hand.
}
\details{
Tournament is a model comparison method that uses WAIC to estimate the predictive performance of the four models and select the most appropriate model given the data. The first round of model comparisons sets up model types "gplm" vs. "gplm0" and "plm" vs. "plm0". In both comparisons, if the more complex model ("gplm" and "plm", respectively) exceeds the \code{winning_criteria} (default value = 1.5) then it is chosen as the more appropriate model and moves on to the second and final round, where the winners from the first round will be compared in the same way. In the second round, if the more complex model (now the generalized power-law model) exceeds the same winning criteria then it is chosen as the overall tournament winner and deemed the most appropriate model given the data.

The default method "WAIC", or the Widely Applicable Information Criterion (see Watanabe (2010)), is used to estimate the predictive performance of the models. This method is a fully Bayesian method that uses the full set of posterior draws to calculate the best possible estimate of the expected log pointwise predictive density.

Method "DIC", or Deviance Information Criterion (see Spiegelhalter (2002)), is similar to the "WAIC" but instead of using the full set of posterior draws to compute the estimate of the expected log pointwise predictive density it uses a point estimate of the posterior distribution.

Method "Posterior_probability" uses the posterior probabilities of the models, calculated with Bayes factor (see Jeffreys (1961) and Kass and Raftery (1995)), to compare the models, where all the models are assumed a priori to be equally likely.

When method "WAIC" or "DIC" is used the \code{winning_criteria} should be a real number. The winning criteria is a threshold value which the more complex model in each model comparison must exceed for it to be declared the more appropriate model. Setting the winning criteria slightly above 0 (default value = 1.5 for both "WAIC" and "DIC") gives the less complex model in each comparison a slight advantage. When method "Posterior_probability" is used the winning criteria should be a real value between 0 and 1 (default value = 0.75). This sets the threshold value for which the posterior probability of the more complex model, given the data, in each model comparison, must exceed for it to be declared the more appropriate model. In all three cases the default value is selected so as to give the less complex models a slight advantage and should give more or less consistent results when applying the tournament to real world data.
}
\examples{
\donttest{
data(krokfors)
set.seed(1)
t_obj <- tournament(formula=Q~W,data=krokfors,num_cores=2)
t_obj
summary(t_obj)
}
}
\references{
Hrafnkelsson, B., Sigurdarson, H., and Gardarsson, S. M. (2022). Generalization of the power-law rating curve using hydrodynamic theory and Bayesian hierarchical modeling, Environmetrics, 33(2):e2711.

Jeffreys, H. (1961). Theory of Probability, Third Edition. Oxford University Press.

Kass, R., and A. Raftery, A. (1995). Bayes Factors. Journal of the American Statistical Association, 90, 773-795.

Spiegelhalter, D., Best, N., Carlin, B., Van Der Linde, A. (2002). Bayesian measures of model complexity and fit. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 64(4), 583–639.

Watanabe, S. (2010). Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory. J. Mach. Learn. Res. 11, 3571–3594.
}
\seealso{
\code{\link{summary.tournament}} and \code{\link{plot.tournament}}
}
