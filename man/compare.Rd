% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model_comparison.R
\name{compare}
\alias{compare}
\title{Compare Bayesian Discharge Rating Curve Models}
\usage{
compare(...)
}
\arguments{
\item{...}{Two or more model objects of class "plm0", "plm", "gplm0", or "gplm".}
}
\value{
An object of class "compare" containing:
  \item{comparison}{A data frame with model comparison metrics}

The returned data frame has the following columns:
  \itemize{
    \item \code{name}: The name of the model.
    \item \code{WAIC}: The WAIC estimate for the model.
    \item \code{SE}: The standard error of the WAIC estimate.
    \item \code{dWAIC}: The difference between the WAIC of this model and the WAIC of the best model (lowest WAIC).
    \item \code{dSE}: The standard error of the WAIC difference (NA for the best model).
    \item \code{pWAIC}: The effective number of parameters.
    \item \code{weight}: The model weight, computed with WAIC.
  }

The models are ordered from best (lowest WAIC) to worst (highest WAIC).
}
\description{
This function compares multiple Bayesian discharge rating curve models using
the Widely Applicable Information Criterion (WAIC).
}
\details{
The \code{compare} function uses the WAIC to estimate and compare the expected out-of-sample prediction error of the models (Watanabe, 2010; Gelman et al., 2013).

The WAIC, which quantifies the trade-off between fit and complexity, is founded in information theory and based on two quantities: the log pointwise predictive density (lppd), a goodness-of-fit measure; and a penalization term referred to as the effective number of parameters (\eqn{p_{\text{waic}}}).

The log pointwise predictive density (lppd) can be estimated as
\deqn{\widehat{\text{lppd}} = \sum_{i=1}^{n} \log(\frac{1}{L}\sum_{l=1}^{L}p(y_i|\boldsymbol{\psi}^{(l)}))\hspace{1mm},}

and the effective number of parameters (pWAIC) can be estimated as
\deqn{\widehat{p}_{\text{waic}} = \sum_{i=1}^{n} (\frac{1}{L-1}\sum_{l=1}^{L}(\log p(y_i|\boldsymbol{\psi}^{(l)}) - \bar{p}_i)^2)\hspace{1mm},}

where \eqn{\boldsymbol{\psi}^{(l)}} is the \eqn{l}-th posterior sample, \eqn{p(y_i|\boldsymbol{\psi}^{(l)})} is the likelihood of the \eqn{i}-th observation,
and \eqn{\bar{p}_i} is the average of \eqn{\log p(y_i|\boldsymbol{\psi}^{(l)})} over all posterior samples.

The estimated expected log pointwise predictive density for WAIC is defined as
\deqn{\widehat{\text{elppd}}_{\text{waic}} = \widehat{\text{lppd}} - \widehat{p}_{\text{waic}}\hspace{1mm}.}

By transforming this quantity onto the deviance scale, you get WAIC:
\deqn{\text{WAIC} = -2\hspace{1mm} \widehat{\text{elppd}}_{\text{waic}}\hspace{1mm}.}

The standard error of the WAIC estimate (SE) can be computed as
\deqn{\text{se(WAIC)}=\sqrt{nV_{i=1}^{n}\text{WAIC}_i}\hspace{1mm},}

where \eqn{\text{WAIC}:=\sum_{i=1}^{n}\text{WAIC}_i}, and \eqn{V_{i=1}^{n}} denotes the sample variance.

For the model comparison, we are mainly interested in the WAIC difference (dWAIC) between models,

 \deqn{\Delta_{\text{waic}} = \text{WAIC}^\text{B} - \text{WAIC}^\text{A}\hspace{1mm},}

where \eqn{\text{A}}, \eqn{\text{B}} are the models being compared.

The standard error of this difference (dSE) can be estimated as:
\deqn{\text{se}(\Delta_{\text{waic}}) = \sqrt{n V_{i=1}^{n}(\text{WAIC}_i^\text{B} - \text{WAIC}_i^\text{A})}\hspace{1mm}.}

Lastly, WAIC is used to compute model weights (e.g., Burnham and Anderson, 2002). The weights provide a measure of the relative likelihood of each model given the data. The weight for each model is calculated as:

\deqn{w_k = \frac{\exp(-0.5 \Delta_{\text{waic},k})}{\sum_{m=1}^M \exp(-0.5 \Delta_{\text{waic},m})}\hspace{1mm},}

where \eqn{M} is the total number of models being compared, and \eqn{\Delta_{\text{waic},k}} is the difference between the WAIC of model \eqn{k} and the WAIC of the best model, where \eqn{k\in\{1,...,M\}}.
}
\examples{
\dontrun{
set.seed(1)
data(krokfors)
plm0.fit <- plm0(Q ~ W, krokfors, num_cores = 2)
gplm.fit <- gplm(Q ~ W, krokfors, num_cores = 2)
compare(plm0.fit, gplm.fit)
# Model Comparison (WAIC)
#
#     name  WAIC   SE dWAIC dSE pWAIC weight
# gplm.fit -28.0 11.9     0  NA   6.8      1
# plm0.fit  -2.9  6.7    25 9.1   4.1      0
}
}
\references{
Burnham, K. P. and Anderson, D. R. (eds) (2002). Information and Likelihood Theory: A Basis for Model Selection and Inference. New York, NY: Springer New York. 49â€“97, doi:https://doi.org/10.1007/978-0-387-22456-5_2.

Gelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari, A. and Rubin, D. (2013). Bayesian Data Analysis. Chapman & Hall/CRC, 3rd ed., doi:https://doi.org/10.1201/b16018.

Watanabe, S. (2010). Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory. Journal of Machine Learning Research, 11, 3571-3594.
}
