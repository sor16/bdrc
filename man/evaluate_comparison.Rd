% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tournament.R
\name{evaluate_comparison}
\alias{evaluate_comparison}
\title{Compare two models using a specified model-selection criteria}
\usage{
evaluate_comparison(m, method, winning_criteria)
}
\arguments{
\item{m}{A list of two model objects fit on the same dataset. The allowed model objects are "gplm", "gplm0", "plm" and "plm0"}

\item{method}{A string specifying the method used to estimate the predictive performance of the models. The allowed methods are "WAIC", "DIC" and "PMP".}

\item{winning_criteria}{A numerical value which sets the threshold which the first model in the list must exceed for it to be declared the more appropriate model. This value defaults to 2 for methods "WAIC" and "DIC", but defaults to 0.75 for method "PMP".}
}
\value{
A data.frame with the summary of the results of each comparison
}
\description{
evaluate_comparison uses the Widely Applicable Information Criterion (WAIC), the Deviance Information Criterion (DIC), or the posterior model probabilities (PMP), calculated with Bayes factor, to determine whether one model is more appropriate than the other given the data at hand.
}
\references{
Hrafnkelsson, B., Sigurdarson, H., Rögnvaldsson, S., Jansson, A. Ö., Vias, R. D., and Gardarsson, S. M. (2022). Generalization of the power-law rating curve using hydrodynamic theory and Bayesian hierarchical modeling, Environmetrics, 33(2):e2711. doi: https://doi.org/10.1002/env.2711
}
\seealso{
\code{\link{tournament}}
}
\keyword{internal}
